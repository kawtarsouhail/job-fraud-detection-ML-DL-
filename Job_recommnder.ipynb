{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDSxpdfcnoapW6tlQLPrWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kawtarsouhail/job-fraud-detection-ML-DL-/blob/main/Job_recommnder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Business Problem\n",
        "This project aims to detect fraudulent job postings and recommend relevant real jobs using NLP techniques.\n"
      ],
      "metadata": {
        "id": "NfLVhDwVBWy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Understanding"
      ],
      "metadata": {
        "id": "qlf00ZwU05Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"victor/real-or-fake-fake-jobposting-prediction\")\n",
        "dataset = ds['train']\n",
        "import pandas as pd\n",
        "df = dataset.to_pandas()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PJX9OPAEKhwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning\n"
      ],
      "metadata": {
        "id": "Wg49RzLmB3ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicates\n",
        "df = df.drop(columns=['job_id'])\n",
        "df = df.drop_duplicates()\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZiBTFabOl8y",
        "outputId": "9a914234-818a-49cf-ae72-a8f58f530c34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dataset using wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# on the basis\n",
        "job_titles_text = ' '.join(df['title'])\n",
        "wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='white').generate(job_titles_text)\n",
        "\n",
        "# plotting Word cloud\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud,interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sKX1CaaEQ78l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_titles_counts = df['title'].value_counts()\n",
        "\n",
        "# plotting the top 10 job titles\n",
        "\n",
        "top_job_tls = job_titles_counts.head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_job_tls.sort_values().plot(kind='barh')\n",
        "plt.title('Top 10 Common Job Titles')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Job Titles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3LpJxH76S28p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering\n"
      ],
      "metadata": {
        "id": "BJc0PvndWufF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to concatenate\n",
        "columns_to_concat = ['title','location','department','salary_range', 'company_profile','description','requirements','benefits','employment_type',\n",
        "'required_experience','required_education','industry','function']\n",
        "\n",
        "# Concatenate the values of specified columns into a new column 'job_posting'\n",
        "df['job_posting'] = df[columns_to_concat].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Create a new DataFrame with columns 'job_posting' and 'fraudulent'\n",
        "new_df = df[['job_posting', 'fraudulent']].copy()"
      ],
      "metadata": {
        "id": "cEZVrAeNW7Tj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pre_processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove newlines, carriage returns, and tabs\n",
        "    text = re.sub('\\n','', text)\n",
        "    text = re.sub('\\r','', text)\n",
        "    text = re.sub('\\t','', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "new_df['job_posting'] = new_df['job_posting'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "XsN5L581XIsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Lemmatization - It is a process to reduce a word to its root form, called a lemma.\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "new_df['job_posting'] = new_df['job_posting'].apply(lemmatize_text)\n"
      ],
      "metadata": {
        "id": "T46uOI75XYHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud job posting removal using LSTM\n"
      ],
      "metadata": {
        "id": "Dg41S2HeYd-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "CO4Z6tWDX3vi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = new_df['job_posting']\n",
        "y = new_df['fraudulent']"
      ],
      "metadata": {
        "id": "-nUUiNeSbXHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE hot coding\n",
        "voc_size = 5000\n",
        "onehot_repr = [one_hot(words, voc_size) for words in X]\n",
        "\n",
        "sent_length = 40\n",
        "X_seq = pad_sequences(onehot_repr, padding='pre', maxlen=sent_length)\n"
      ],
      "metadata": {
        "id": "YyoyzrYTbdNp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Creating\n"
      ],
      "metadata": {
        "id": "Wm4mkKSgCRrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the model\n",
        "embedding_vector_features = 50\n",
        "model = Sequential()\n",
        "model.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "LwmMMHdbbn4c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split the data to train data and test data"
      ],
      "metadata": {
        "id": "i03YbmVVfRcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_final = np.array(X_seq)\n",
        "y_final = np.array(y)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train-test split for LSTM\n",
        "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
        "    X_seq, y, test_size=0.25, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "y2BeZqrifYR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq), epochs=2, batch_size=64)\n"
      ],
      "metadata": {
        "id": "vH4svGuJfNvF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map the [0,1] to 0 or 1\n",
        "y_pred_lstm = (model.predict(X_test_seq) > 0.5).astype(\"int32\").flatten()"
      ],
      "metadata": {
        "id": "FV1PpjIhgHEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== LOGISTIC REGRESSION MODEL ==========\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')\n",
        "X_tfidf = tfidf.fit_transform(X)\n",
        "\n",
        "# Train-test split for Logistic Regression\n",
        "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
        "    X_tfidf, y, test_size=0.25, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "qVBEGL6jEG8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Train model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr_model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "j44rcjlOGQhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Evaluation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmrRRLmLgYvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"========== MODEL COMPARISON ==========\")\n",
        "\n",
        "# LSTM metrics\n",
        "lstm_acc = accuracy_score(y_test_seq, y_pred_lstm)\n",
        "lstm_f1 = f1_score(y_test_seq, y_pred_lstm)\n",
        "\n",
        "# Logistic Regression metrics\n",
        "lr_acc = accuracy_score(y_test_tfidf, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test_tfidf, y_pred_lr)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc:.4f}\")\n",
        "print(f\"LSTM F1-score: {lstm_f1:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
        "print(f\"Logistic Regression F1-score: {lr_f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix - LSTM\n",
        "print(\"\\nLSTM Confusion Matrix:\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test_seq, y_pred_lstm)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix - Logistic Regression\n",
        "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test_tfidf, y_pred_lr)\n",
        "plt.show()\n",
        "\n",
        "# Logistic Regression full report\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\")\n",
        "print(classification_report(y_test_tfidf, y_pred_lr))\n"
      ],
      "metadata": {
        "id": "brbV78adIBQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 7. Job Recommendation System\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VGCCw5Tvi8oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== JOB RECOMMENDATION SYSTEM ==========\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Filter real jobs\n",
        "real = df[df['fraudulent'] == 0].fillna('')\n",
        "real['text'] = real['description'] + \" \" + real['requirements'] + \" \" + \\\n",
        "               real['required_experience'] + \" \" + real['required_education'] + \" \" + real['industry']\n",
        "\n",
        "# Load model\n",
        "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings\n",
        "emb_matrix = sentence_model.encode(real['text'].tolist())"
      ],
      "metadata": {
        "id": "8ETjoE8FSsNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smart_job_recommendation(user_input, top_n=5):\n",
        "    \"\"\"\n",
        "    Universal Job Recommendation Function\n",
        "    Works with:\n",
        "    - Job titles\n",
        "    - Keywords\n",
        "    - Full profile descriptions\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Create combined text (title + description)\n",
        "    if 'combined_text' not in real.columns:\n",
        "        real['combined_text'] = real['title'].fillna('') + \" \" + real['description'].fillna('')\n",
        "\n",
        "    # 2. Encode combined job data only once\n",
        "    global emb_matrix\n",
        "    if 'emb_matrix' not in globals():\n",
        "        emb_matrix = sentence_model.encode(\n",
        "            real['combined_text'].tolist(),\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "    # 3. Encode user input\n",
        "    user_embedding = sentence_model.encode([user_input])\n",
        "\n",
        "    # 4. Compute cosine similarity\n",
        "    similarities = cosine_similarity(user_embedding, emb_matrix)[0]\n",
        "\n",
        "    # 5. Create result dataframe\n",
        "    results_df = real.copy()\n",
        "    results_df['similarity_score'] = similarities\n",
        "\n",
        "    # 6. Sort and return top N results\n",
        "    results_df = results_df.sort_values(by='similarity_score', ascending=False).head(top_n)\n",
        "\n",
        "    return results_df[['title', 'location', 'industry', 'similarity_score']]\n",
        "\n",
        "\n",
        "\n",
        "# Recommendation by user input\n",
        "def recommend_by_profile(user_text, top_n=5):\n",
        "    user_embedding = sentence_model.encode([user_text])\n",
        "    similarities = cosine_similarity(user_embedding, emb_matrix)[0]\n",
        "    real['similarity_score'] = similarities\n",
        "    return real.sort_values(by='similarity_score', ascending=False).head(top_n)[\n",
        "        ['title','location','industry','similarity_score']\n",
        "    ]"
      ],
      "metadata": {
        "id": "k9DWIWAZlNeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smart_job_recommendation(\"ML Developer\", top_n=5)\n"
      ],
      "metadata": {
        "id": "i_FPQTWqbmRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_by_profile(\"Backend developer with Python and Django\",10)"
      ],
      "metadata": {
        "id": "Q3VVoKkPoWIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(tfidf, \"/content/tfidf_vectorizer.pkl\")\n",
        "joblib.dump(lr_model, \"/content/logistic_model.pkl\")\n",
        "\n",
        "# Si ton LSTM est bien entraîné :\n",
        "model.save(\"/content/lstm_fraud_model.keras\")\n"
      ],
      "metadata": {
        "id": "O3xFlJHMGmYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/models.zip \\\n",
        "/content/lstm_fraud_model.keras \\\n",
        "/content/tfidf_vectorizer.pkl \\\n",
        "/content/logistic_model.pkl\n"
      ],
      "metadata": {
        "id": "kLgb73xeLYCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/models.zip\")"
      ],
      "metadata": {
        "id": "CZAFczPYMhhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}